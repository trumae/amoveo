[
  { amoveo_core, [

    %% comment

    {cores_to_mine, 1000}, %% The maximum number of cores to use when mining.

    {channel_delay, 10},
    {max_message_size, 10000},
    {token_decimals, 100000000},

    %% Everything above this line is needed for blockchain consensus. Everything below this line can be modified for your node.

    {files, "../../../../db/"}, %% this is where things like blocks and accounts get stored
    %% {files, "/home/CHANGE-TO-YOUR-USERNAME/.amoveo/"},
    {white_list, [{127,0,0,1}]}, %% This maintains a list of node who we allow to get information from us as frequently as we can respond. 
    {channels, false},
    {sharding, full_node}, %%full_node, light_node
    {smart_contract_runtime_limit, 5000}, %% in miliseconds.

    {recent_blocks_period, 3}, %% this is how frequently we calculate which blocks should no longer be tracked for pruning.
    %% {prune_period, 3}, %% this is how frequently in blocks we prune all the data from the trees from before revert_depth.
    %% {prune_txs, 30}, %% this is how frequently in txs we prune all the data from the trees before revert_depth
    %% if prune period is low, then you will use less ram, but you will waste more cpu on pruning.
     %% {revert_depth, 20}, %% save all data from the most recent block, and this far into history. That way if blocks are reverted, we still have all the state. For light nodes this should be set to 0.
     %%    {revert_depth, 100000000}, %% save all data from the most recent block, and this far into history. That way if blocks are reverted, we still have all the state. For light nodes this should be set to 0.
    %% if a fork is bigger than revert_depth, your full node will have to reprocess all the blocks, or it will have to download a bunch of proofs.
    {confirmations_needed, 0}, %% if you want to make a bet with this server, your channel needs at least this many confirmations.

    {light_node, false},
    {trie_size, 3000000}, %%  we can adjust this many accounts and channels in all the blocks in revert_depth. So it is important that the number of proofs per block * revert_depth is smaller than this number. 
    %% There are several trees, they are all stored in ram. It is important that trie_size*sum(size of an element in each trie) < (amount of ram you have).

    {tx_fee, 1000},
    {lightning_fee, 9},
    {minimum_tx_fee, 9}, %% Only txs with this fee or higher get accepted into your mempool. If you are a miner, you are censoring all txs with lower fees.

    {fork_tolerance, 50}, %% This is used by the channel manager. If the channel has been closed this many blocks, then the channel managers garbage collection would delete it.
                     %% it is also used when syncing. You download this many extra headers to check if you are on a fork.
		     %% also used in block_hashes.erl. this is involved in calculating how many recent block hashes to keep to know which blocks you have already verified.
    {headers_batch, 5001}, %% You download up to this many headers per request.

    {time_value, 1250}, %% This is / (10 expt 8)
    %% there are around 8000 blocks a month. This node is designed to double its balance at least every 10 months, if it is working at full capacity. It is expected to double its money every 5 months.
    %% the customer is paying about 10% of the money in the channel per month as a fee.
    %% 1250.0 == (math:pow(10, 8) div 80000).
    {min_channel_fee, 100000000},
    {min_channel_ratio, 0.5}, %% So the customer needs to put in twice as much money as the server.

    {bet_gas_limit, 100000},%% these limits are for when you are making channels off-chain. you are avoiding making a channel that would be excessively expensive to close on-chain.
    {time_limit, 100000}, %% Maximum amount of time to wait for a channel contract to process.
    {space_limit, 100000},
    {fun_limit, 1000}, %%it is recommended to keep the fun and var limits below what is allowed in governance. So if the governance value was to change, your channel will still be valid.
    {var_limit, 10000},

    {min_channel_delay, 99}, %% Needs to be long enough for you to stop your partner from closing at the wrong state.
    {max_channel_delay, 2000},
    {min_channel_lifespan, 30}, %%Minimum amount of time you can get a channel with this node for. measured in blocks.

    {download_blocks_batch, 50},%% How many blocks we get per request.
    {download_blocks_many, 3}, %% we are storing blocks in the block_absorber mail box, which is RAM. many*batch*blocksize is how much ram we are using, at most.
    {headers_version, 2},
    {trie_mode, hd},%hd or ram. hd mode saves like 5 mb of ram, but is slower.
    {db_version, 2},%version 1 stores each block compressed as its own page on the hard drive. version 2 stores the recent blocks in ram, and stores pages of hundreds of older blocks compressed together. When we are serving the blocks, version 2 sends entire pages together without decompressing them, saving a lot of bandwidth.
    {blocks_per_page, 1000},%When we compress blocks to store on the hard drive, we do it in pages so that the compression algorithm has lots of repeated data to compress. if you change this number, then you will need to resync all the blocks, because your node wont understand how to read the blocks you already have from your hard drive.
    {block_cache, 5000000}, %5 megabytes, this is how many blocks measured in bytes in ram get compressed into a page of blocks on the hard drive.
    {get_block_buffer, 10},%how many blocks to download and store in a queue ready to be synced. Increasing this takes more ram, and makes some data structures slower, but it can help if your internet connection has high latency.
    {compression_level, 2},%integer between 1 and 9, 9 is the most compression.
    {block_threads, 1},
    {push_blocks_batch, 50},

    {checkpoint_depth, 1000},%we want to keep this bigger than the fork_tolerance.
    {reverse_syncing, true},
    {minimum_to_verify, 162000}, %below this block height, it does not verify blocks, it just checks that the blocks hash matches the header for the same height.

    {internal_ip, {127,0,0,1}},
    {external_ip, {0,0,0,0}},
    {pool_refresh_period, 600},
    {pool_refresh_script, "../../../../do_nothing.sh"},
    {block_meta, false},
    {block_meta_block, true},
    {block_meta_txs, true},
    {block_meta_governance, true},
    {block_meta_before, true},
    {block_meta_following, true},
    
    {mining_pool_refresher, 0},

    {internal_tx_timeout, 2000}, %%in miliseconds.

    {assume_valid, {215000, <<106,76,110,178,197,122,84,1,2,214,133,241,162,40,199,195,154,209,75,247,164,238,62,15,204,26,49,163,115,111,239,85>>}}
    %200 blocks per second without this.
    %starts at height: 100 time: 161438 880120

  ]},

   {lager, [
       {handlers, [
          {lager_file_backend, [{file, "log/testnet.log"}, {level, debug}, {size, 41943040}, {date, "$D0"}, {count, 10}]}
      ]}
  ]}
].